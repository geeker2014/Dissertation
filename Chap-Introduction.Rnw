% !Rnw root = Master.Rnw

\chapter{Introduction}
\label{sec:intro}
An important part of statistics is building mathematical models to help describe the relationship between variables. Often, we have a dependent variable $\mathcal{Y}$, called the response, and an independent variable $\mathcal{X}$, called the predictor. The researcher designs an experiment and procures $n$ observed pairs $(x_1, y_1), \dotsc, (x_n, y_n)$ that are then used to fit a regression model. The fitted regression model is often used to make predictions. In particular,
\begin{itemize}
  \item predict an individual response for a given value of the predictor;
  \item estimate the mean response for a given value of the predictor.
\end{itemize}
Sometimes, however, the researcher is interested in the reverse problem. That is, there is a need to estimate the predictor value from an observed value of the response (\textit{calibration}) or a specified value of the mean response (\textit{regulation}). Both are referred to more generally as \textit{inverse estimation}. Calibration has also been referred to in the literature as \textit{inverse prediction}, \textit{inverse regression}, and \textit{discrimination}. In this paper, we discuss calibration, in particular, univariate calibration. For an overview on topics in multivariate calibration, see \citet{brown_multivariate_1982}, \citet{brown_confidence_1987}, and \citet{brown_measurement_1993}. Point estimation is reviewed in Section~\ref{sec:point-estimation}, interval estimation in Section~\ref{sec:interval-estimation}, and Bayesian calibration in Section~\ref{sec:bayesian}.   

A calibration experiment typically consists of two stages. In the first stage, $n$ observations $(x_i, y_i)$ are collected (henceforth referred to as the \textit{standards}) and used to fit a regression model $\E\left\{\mc{Y} | x\right\} = \mu$. The fitted model, denoted $\wh{\mu}$, is often referred to as the \textit{calibration curve} or \textit{standards curve}. In the second stage, $m$ ($m \ge 1$) values of the response are observed with unknown predictor value $x_0$ (henceforth referred to as the \textit{unknowns}). The goal is to use the calibration curve $\wh{\mu}$ to estimate $x_0$. The following example will help to clarify the basic idea. 

Suppose a new procedure has been developed for measuring the concentration (in $\mu$g/ml) of arsenic in water samples that is cheaper, but less accurate than the existing method. An investigator has procured 32 water samples containing preselected amounts of arsenic $x_i$ and subjected them to the new method producing measured concentrations $y_i$. The standards, taken from \citet{graybill_regression_1994}, are plotted in Figure~\ref{fig:arsenic-scatter}. A new water sample is then obtained with unknown arsenic concentration $x_0$ and subjected to the new method, producing a measured concentration of 3.0 $\mu$g/ml. It is desired to estimate the true concentration of arsenic in the newly obtained water sample. 

<<arsenic-scatter, echo=FALSE, fig.width=5, fig.height=4, par=TRUE, fig.pos='!tbh', fig.scap='Scatterplot of the arsenic data', fig.cap='Scatterplot of the arsenic data.'>>=
plot(measured ~ actual, data = arsenic,      
     xlab = expression(paste("Actual (", mu, "g/ml)")), 
     ylab = expression(paste("Measured (", mu, "g/ml)")))
@

The goal of this work is to extend the methods of calibration to more complicated settings. In Chapter~\ref{chap:nonparametric}, we introduce \textit{semiparametric calibration}. Here we use methods of semiparametric regression to estimate the calibration curve and make inference on the unknown $x_0$. The benefit of this approach is that we do not have to specify the exact form of the calibration curve. The downside to this approach is that the estimated calibration curve will be biased, hence, our inference about the unknown $x_0$ will also be biased. We correct for this bias by taking the mixed model approach to smoothing described in, for example, \citet{ruppert_semiparametric_2003}. A small Monte Carlo study shows that this correction is necessary to obtain calibration intervals with coverage probability near the nominal $1-\alpha$ level. We also extend the method of Bayesian linear calibration put forth by \citet{hoadley_bayesian_1970} by allowing for the calibration curve to be estimated semiparametrically as in \citet{crainiceanu_bayesian_2005}. In Chapter~\ref{chp:cal-dependent}, we extend the classical methods of calibration to work with linear mixed-effects models. We also propose a new parametric bootstrap algorithm that can be used to obtain an estimate of the entire sampling distribution of the estimate of $x_0$. We further show how this algorithm can also be used to improve upon the classical methods by removing normality assumptions that may not be satisfied in practice. We use several real datasets to demonstrate and compare our methods.

%This is similar (in spirit) to the parametric bootstrap adjustment proposed by \citet{oman_calibration_1998}, however, Oman's approach is only approximate and does not seem to account for the added variability from the observed response corresponding to the unknown $x_0$.